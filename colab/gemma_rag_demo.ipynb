{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph-RAG with Gemma 3 1B IT (Colab)\n",
        "\n",
        "This notebook demonstrates end-to-end retrieval that merges semantic neighbors (Chroma) with call-graph context (Neo4j via Joern CPG).\n",
        "\n",
        "Prereqs:\n",
        "- Accept the Hugging Face license for `google/gemma-3-1b-it`.\n",
        "- Have a Neo4j instance with Joern CSVs ingested.\n",
        "- Have a Chroma collection built from the repo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "pip -q install transformers accelerate torch neo4j chromadb sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from neo4j import GraphDatabase\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "HF_TOKEN = os.getenv('HF_TOKEN', '')\n",
        "if not HF_TOKEN:\n",
        "    print('Note: Set HF_TOKEN to use gated models if needed.')\n",
        "\n",
        "# Connect to Chroma (adjust if remote)\n",
        "CHROMA_PATH = os.getenv('CHROMA_PATH', '/content/chroma_store')\n",
        "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH, settings=Settings(anonymized_telemetry=False))\n",
        "collection = chroma_client.get_collection('repo_chunks')\n",
        "\n",
        "# Connect to Neo4j\n",
        "NEO4J_URI = os.getenv('NEO4J_URI', 'bolt://localhost:7687')\n",
        "NEO4J_USER = os.getenv('NEO4J_USER', 'neo4j')\n",
        "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD', 'test-password')\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "# Load Gemma\n",
        "model_id = 'google/gemma-3-1b-it'\n",
        "print('Loading model:', model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN if HF_TOKEN else None)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype='auto', use_auth_token=HF_TOKEN if HF_TOKEN else None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from repo_indexer.graph.query_graph import get_functions_for_chunk, get_call_subgraph, serialize_graph_for_model\n",
        "\n",
        "\n",
        "def build_prompt(graph_text: str, semantic_chunks: list, question: str) -> str:\n",
        "    lines = []\n",
        "    lines.append('System:\\nYou are a code reasoning assistant. Use the code snippets and the call graph to answer questions concisely. When giving step-by-step flow, number the steps. When referring to code, include filepath:line ranges.')\n",
        "    lines.append('\\nContext:\\n[CALL GRAPH]\\n' + graph_text)\n",
        "    lines.append('\\n[SEMANTIC CHUNKS]')\n",
        "    for i, c in enumerate(semantic_chunks, 1):\n",
        "        loc = f\"{c.get('filepath')}:{c.get('lines',[\"?\",\"?\"])[0]}-{c.get('lines',[\"?\",\"?\"])[1]}\"\n",
        "        lines.append(f\"- {i}) {c.get('summary')} ({loc})\\n{c.get('snippet','')[:400]}\")\n",
        "    lines.append('\\nTask:\\n' + question)\n",
        "    lines.append('\\nResponse format:\\n- Short answer (1-2 lines)\\n- Key steps (numbered)\\n- Relevant code pointers (filepath:lines)\\n- If unsure, state assumptions.')\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "def run_inference(prompt: str, max_new_tokens: int = 256, temperature: float = 0.2):\n",
        "    import torch\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: take first chunk from chunks.jsonl and run retrieval\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "chunks_path = Path('/content/repo-indexer/outputs/chunks.jsonl')\n",
        "if not chunks_path.exists():\n",
        "    print('Note: mount your repo outputs to /content/repo-indexer/outputs/chunks.jsonl')\n",
        "\n",
        "chunk = None\n",
        "if chunks_path.exists():\n",
        "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
        "        line = f.readline()\n",
        "        if line:\n",
        "            item = json.loads(line)\n",
        "            chunk = {'document': item.get('text',''), 'metadata': item.get('metadata',{})}\n",
        "\n",
        "if not chunk:\n",
        "    chunk = {'document': 'def foo():\\n  return 1', 'metadata': {'summary': 'demo function foo'}}\n",
        "\n",
        "# Semantic neighbors via a quick embedding using sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "emb = model.encode([chunk['metadata'].get('summary') or chunk['document']], convert_to_tensor=False)[0]\n",
        "results = collection.query(query_embeddings=[emb.tolist()], n_results=3, include=['documents','metadatas'])\n",
        "semantic = []\n",
        "for i in range(len(results['ids'][0])):\n",
        "    semantic.append({\n",
        "        'id': results['ids'][0][i],\n",
        "        'filepath': results['metadatas'][0][i].get('filepath'),\n",
        "        'summary': results['metadatas'][0][i].get('summary'),\n",
        "        'language': results['metadatas'][0][i].get('language'),\n",
        "        'node_type': results['metadatas'][0][i].get('node_type'),\n",
        "        'lines': [results['metadatas'][0][i].get('start_line'), results['metadatas'][0][i].get('end_line')],\n",
        "        'snippet': results['documents'][0][i][:800]\n",
        "    })\n",
        "\n",
        "# Graph subgraph\n",
        "with driver.session() as session:\n",
        "    fnames = get_functions_for_chunk(chunk)\n",
        "    nodes, edges = get_call_subgraph(session, fnames, direction='both', hops=2)\n",
        "    graph_text = serialize_graph_for_model(nodes, edges, max_tokens=600)\n",
        "\n",
        "prompt = build_prompt(graph_text, semantic, 'Explain the flow and purpose of the selected code.')\n",
        "print(prompt[:1000])\n",
        "\n",
        "output = run_inference(prompt)\n",
        "print('\\n--- Model Output ---\\n')\n",
        "print(output)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
